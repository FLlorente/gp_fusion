{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import rdata\n",
    "from sklearn.model_selection import train_test_split\n",
    "import arviz as az\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "from numpyro.infer import NUTS, MCMC, SVI, Trace_ELBO, Predictive\n",
    "from numpyro.infer.autoguide import AutoDelta, AutoNormal\n",
    "from optax import adam, chain, clip\n",
    "\n",
    "from numpyro import distributions as dist\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from jax.scipy import linalg\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numpyro.set_host_device_count(4)\n",
    "numpyro.set_platform(\"cpu\")\n",
    "numpyro.enable_x64()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCI regression datasets with less than 1000 observations\n",
    "\n",
    "from uci_datasets import all_datasets\n",
    "[name for name, (n_observations, n_dimensions) in all_datasets.items() if n_observations < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uci_datasets import Dataset\n",
    "# data = Dataset('yacht')\n",
    "# data = Dataset('stock')\n",
    "data = Dataset('concrete')\n",
    "# data = Dataset('parkinsons')\n",
    "# data = Dataset('elevators')\n",
    "# data = Dataset(\"airfoil\")\n",
    "\n",
    "X_train, y_train, X_test, y_test = data.get_split(split=5)  # there are 10 different trainning-test splits\n",
    "\n",
    "y_train = y_train.squeeze(); y_test = y_test.squeeze()\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data \n",
    "X_train_max = X_train.max(0)\n",
    "X_train_min = X_train.min(0)\n",
    "X_train = (X_train - X_train_min) / (X_train_max - X_train_min)\n",
    "X_test = (X_test - X_train_min) / (X_train_max - X_train_min)\n",
    "\n",
    "y_train_mean = y_train.mean()\n",
    "y_train_std = y_train.std()\n",
    "\n",
    "y_train = (y_train - y_train_mean)/y_train_std\n",
    "y_test = (y_test - y_train_mean)/y_train_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script true\n",
    "print(model.covar_module.base_kernel.lengthscale.detach().numpy())\n",
    "print(model.covar_module.outputscale.detach().numpy())\n",
    "print(model.likelihood.noise_covar.noise.detach().numpy())\n",
    "print(model.mean_module.constant.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_single_gp(X_train,y_train,X_test,X_val,kappa,lambdaa):\n",
    "    Xtrain_torch = torch.from_numpy(X_train).type(torch.float32)\n",
    "    Ytrain_torch = torch.from_numpy(y_train).type(torch.float32).squeeze(-1)\n",
    "    Xtest_torch = torch.from_numpy(X_test).type(torch.float32)\n",
    "    Xval_torch = torch.from_numpy(X_val).type(torch.float32)\n",
    "\n",
    "    # We will use the simplest form of GP model, exact inference\n",
    "    class ExactGPModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "            lengthscale_prior = gpytorch.priors.GammaPrior(1, 1)\n",
    "            outputscale_prior = gpytorch.priors.GammaPrior(1, 2)\n",
    "\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(\n",
    "                ard_num_dims=Xtrain_torch.shape[1],\n",
    "                lengthscale_prior = lengthscale_prior\n",
    "                )\n",
    "                ,outputscale_prior = outputscale_prior,\n",
    "                )\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    # initialize likelihood and model\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_prior=gpytorch.priors.GammaPrior(1, 1))\n",
    "    model_gpy = ExactGPModel(Xtrain_torch, Ytrain_torch, likelihood)\n",
    "\n",
    "    # hypers = {\n",
    "    #     'likelihood.noise_covar.noise': torch.tensor(0.5),\n",
    "    #     'covar_module.base_kernel.lengthscale': torch.tensor(0.5)*torch.ones(X_train.shape[1],1),\n",
    "    #     'covar_module.outputscale': torch.tensor(1.0),\n",
    "    # }\n",
    "\n",
    "    hypers = {\n",
    "        'likelihood.noise_covar.noise': torch.tensor(1.0*y_train.var()/kappa**2),\n",
    "        'covar_module.base_kernel.lengthscale': torch.from_numpy(np.std(X_train,axis=0)/lambdaa),\n",
    "        'covar_module.outputscale': torch.tensor(1.0*y_train.var()),\n",
    "        'mean_module.constant': torch.tensor(y_train.mean())\n",
    "    }\n",
    "\n",
    "\n",
    "    model_gpy.initialize(**hypers);\n",
    "\n",
    "    # print(model_gpy.mean_module.constant.item())\n",
    "\n",
    "    training_iter = 100\n",
    "\n",
    "    model_gpy.train()\n",
    "    likelihood.train()   \n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model_gpy.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs: - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model_gpy)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model_gpy(Xtrain_torch)  # this is for computing the prior GP model\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, Ytrain_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model_gpy.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # print(model_gpy.mean_module.constant.item())\n",
    "\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        test_preds = likelihood(model_gpy(Xtest_torch))    # prediccion de las ytest (no ftest) ya que estamos usando likelihood()\n",
    "        train_preds = likelihood(model_gpy(Xtrain_torch)); # prediccion de las ytrain (no ftrain) ya que estamos usando likelihood()\n",
    "        val_preds = likelihood(model_gpy(Xval_torch))    # prediccion de las ytest (no ftest) ya que estamos usando likelihood()\n",
    "\n",
    "\n",
    "    return test_preds, train_preds, val_preds\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, Y, n_splits, split_size, with_replacement=True):\n",
    "    np.random.seed(0)\n",
    "    n_samples = X.shape[0]\n",
    "    # split_size = n_samples // n_splits\n",
    "\n",
    "    if with_replacement:\n",
    "        splits = [(X[indices], Y[indices]) for indices in [np.random.choice(n_samples, split_size, replace=True) for _ in range(n_splits)]]\n",
    "    else:\n",
    "        if split_size * n_splits > n_samples:\n",
    "            raise ValueError(\"Cannot split without replacement as there are not enough samples.\")\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        splits = [(X[indices[i * split_size:(i + 1) * split_size]], Y[indices[i * split_size:(i + 1) * split_size]]) for i in range(n_splits)]\n",
    "    \n",
    "    \n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create validation set and experts' predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_validation_set(splits, n_points_per_split):\n",
    "    np.random.seed(0)\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    \n",
    "    # Iterate over each split\n",
    "    for X_split, y_split in splits:\n",
    "        \n",
    "        # Randomly select n_points_per_split indices from the split\n",
    "        selected_indices = np.random.choice(len(X_split), n_points_per_split, replace=False)\n",
    "        \n",
    "        # Add the selected points to the validation set\n",
    "        X_val.append(X_split[selected_indices])\n",
    "        y_val.append(y_split[selected_indices])\n",
    "        \n",
    "        # Optional: Remove the selected points from the original split (if desired)\n",
    "        # X_split = np.delete(X_split, selected_indices, axis=0)\n",
    "        # y_split = np.delete(y_split, selected_indices, axis=0)\n",
    "        \n",
    "    # Combine the validation points from all splits\n",
    "    X_val = np.concatenate(X_val, axis=0)\n",
    "    y_val = np.concatenate(y_val, axis=0)\n",
    "    \n",
    "    return X_val, y_val\n",
    "\n",
    "n_experts = 5\n",
    "n_data_per_expert = X_train.shape[0]//n_experts\n",
    "\n",
    "splits = split_dataset(X_train, y_train, \n",
    "                       n_splits=n_experts, \n",
    "                       split_size=n_data_per_expert, \n",
    "                      #  split_size = 100,\n",
    "                      #  with_replacement=True,  \n",
    "                      with_replacement=False,    # partition\n",
    "                       )\n",
    "\n",
    "n_points_per_split = 10\n",
    "X_val, y_val = create_validation_set(splits, n_points_per_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu_preds_val = np.zeros((X_val.shape[0],len(splits)))\n",
    "std_preds_val = np.zeros((X_val.shape[0],len(splits)))\n",
    "\n",
    "mu_preds_test = np.zeros((X_test.shape[0],len(splits)))\n",
    "std_preds_test = np.zeros((X_test.shape[0],len(splits)))\n",
    "\n",
    "kappa = 2   # in [2,100]\n",
    "lambdaa = 2  # in [1,10]\n",
    "# cmap = matplotlib.colormaps['viridis']\n",
    "for i, (X_split, y_split) in enumerate(splits):\n",
    "   test_preds, _, val_preds = train_and_predict_single_gp(X_train=X_split,\n",
    "                                y_train=y_split,\n",
    "                                X_test=X_test,\n",
    "                                X_val = X_val,\n",
    "                                kappa=kappa,\n",
    "                                lambdaa = lambdaa)\n",
    "   \n",
    "   mu_preds_val[:,i] = val_preds.mean.numpy()\n",
    "   std_preds_val[:,i] = np.sqrt(val_preds.variance.numpy())\n",
    "\n",
    "   mu_preds_test[:,i] = test_preds.mean.numpy()\n",
    "   std_preds_test[:,i] = np.sqrt(test_preds.variance.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute negative test loglikelihood for every expert before the fusion\n",
    "\n",
    "def compute_neg_log_like(mus,stds,y_test):\n",
    "    negloglik = np.zeros((y_test.shape[0],mus.shape[1]))\n",
    "    for i in range(mus.shape[1]):\n",
    "       negloglik[:,i] = -1.0*scipy.stats.norm.logpdf(y_test, mus[:,i],stds[:,i])\n",
    "\n",
    "    return negloglik.mean(0)   \n",
    "\n",
    "nlpd = compute_neg_log_like(mu_preds_test,std_preds_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds, _, _ = train_and_predict_single_gp(\n",
    "                                X_train=X_train,\n",
    "                                y_train=y_train,\n",
    "                                X_test=X_test,\n",
    "                                X_val = X_val,\n",
    "                                kappa=kappa,\n",
    "                                lambdaa=lambdaa)\n",
    "\n",
    "nlpd_single = compute_neg_log_like(test_preds.mean.numpy().reshape(-1,1),\n",
    "                                   np.sqrt(test_preds.variance.numpy().reshape(-1,1)),\n",
    "                                   y_test)\n",
    "nlpd_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gPoEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusion of GP predictions using log-linear pooling with weights computed \"at prediction time\" (a la gPoEs, rBCM, etc.)\n",
    "\n",
    "def product_fusion(mus,stds,X_test):\n",
    "    prec_fused = np.zeros((X_test.shape[0],1))\n",
    "    mean_fused = np.zeros((X_test.shape[0],1))\n",
    "    w_gpoe = np.zeros(mus.shape)\n",
    "    noise = np.var(y_train)/kappa**2   # we are using the (same for all experts) unlearned hyperparameter values\n",
    "    variance = np.var(y_train)\n",
    "    for n, x in enumerate(X_test):\n",
    "\n",
    "        weights = 0.5*(np.log(noise+variance) - np.log(stds[n,:]**2))\n",
    "        weights = weights / np.sum(weights)\n",
    "\n",
    "        precs = 1/stds[n,:]**2\n",
    "\n",
    "        prec_fused[n,:] = weights @ precs\n",
    "        mean_fused[n,:] = weights @ (mus[n,:]*precs) / prec_fused[n,:]\n",
    "\n",
    "        # store weights\n",
    "        w_gpoe[n,:] = weights\n",
    "\n",
    "    return mean_fused, 1/np.sqrt(prec_fused), w_gpoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus_gpoe, stds_gpoe, w_gpoe = product_fusion(mu_preds_test,std_preds_test,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpd_gpoe = compute_neg_log_like(mus_gpoe,\n",
    "                                 stds_gpoe,\n",
    "                                 y_test)\n",
    "nlpd_gpoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(w_gpoe.shape[1]):\n",
    "    plt.plot(w_gpoe[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squared euclidean distance\n",
    "def sqeuclidean_distance(x, y):\n",
    "    return jnp.sum((x - y) ** 2)\n",
    "\n",
    "# distance matrix\n",
    "def cross_covariance(func, x, y):\n",
    "    \"\"\"distance matrix\"\"\"\n",
    "    return jax.vmap(lambda x1: jax.vmap(lambda y1: func(x1, y1))(y))(x)\n",
    "\n",
    "def SE_kernel(X, Y, var, length, noise, jitter=1.0e-6, include_noise=True):\n",
    "    # distance formula\n",
    "    deltaXsq = cross_covariance(\n",
    "        sqeuclidean_distance, X / length, Y / length\n",
    "    )\n",
    "\n",
    "    assert deltaXsq.shape == (X.shape[0], Y.shape[0])\n",
    "\n",
    "    # rbf function\n",
    "    K = var * jnp.exp(-0.5 * deltaXsq)\n",
    "    if include_noise:\n",
    "        K += (noise + jitter) * jnp.eye(X.shape[0])\n",
    "    return K\n",
    "\n",
    "vmap_SE_kernel = jax.vmap(SE_kernel, in_axes=(None, None, 0, 0, 0))\n",
    "\n",
    "def predict_with_mean(\n",
    "    rng_key,\n",
    "    X,\n",
    "    Y,\n",
    "    X_test,\n",
    "    var,\n",
    "    length,\n",
    "    noise,\n",
    "    kernel_func=SE_kernel,\n",
    "    mean_func=lambda x: jnp.zeros(x.shape[0]),\n",
    "):\n",
    "    # compute kernels between train and test data, etc.\n",
    "    k_pp = kernel_func(X_test, X_test, var, length, noise, include_noise=True)\n",
    "    k_pX = kernel_func(X_test, X, var, length, noise, include_noise=False)\n",
    "    k_XX = kernel_func(X, X, var, length, noise, include_noise=True)\n",
    "    K_xx_cho = jax.scipy.linalg.cho_factor(k_XX)\n",
    "    K = k_pp - jnp.matmul(\n",
    "        k_pX, jax.scipy.linalg.cho_solve(K_xx_cho, jnp.transpose(k_pX))\n",
    "    )\n",
    "    sigma_noise = jnp.sqrt(jnp.clip(jnp.diag(K), a_min=0.0)) * jax.random.normal(\n",
    "        rng_key, X_test.shape[:1]\n",
    "    )\n",
    "    mean = mean_func(X_test) + jnp.matmul(\n",
    "        k_pX, jax.scipy.linalg.cho_solve(K_xx_cho, Y - mean_func(X))\n",
    "    )\n",
    "    # we return both the mean function and a sample from the posterior predictive for the\n",
    "    # given set of hyperparameters\n",
    "    return mean, jnp.sqrt(jnp.diag(K))\n",
    "\n",
    "\n",
    "vmapped_pred_with_mean = jax.vmap(\n",
    "    predict_with_mean, in_axes=(None, None, 0, None, 0, 0, 0, None, None)\n",
    ")\n",
    "\n",
    "vmapped_pred_with_mean = jax.vmap(\n",
    "    vmapped_pred_with_mean,\n",
    "    in_axes=(None, None, 1, None, 1, 1, 1, None, None),\n",
    ")\n",
    "\n",
    "def mult_stack_gp_w_model(X, mu_preds, std_preds, y_val=None):\n",
    "    N, M = mu_preds.shape\n",
    "\n",
    "    assert mu_preds.shape == std_preds.shape\n",
    "\n",
    "    tau_preds = 1 / std_preds**2\n",
    "\n",
    "    ######################\n",
    "    # GP for log weights #\n",
    "    ######################\n",
    "    with numpyro.plate(\"M\", M):\n",
    "        kernel_var = numpyro.sample(\"kernel_var\", dist.HalfNormal(1.0))\n",
    "        kernel_length = numpyro.sample(\"kernel_length\", dist.InverseGamma(5.0, 5.0))\n",
    "        kernel_noise = numpyro.sample(\"kernel_noise\", dist.HalfNormal(1.0))\n",
    "\n",
    "    k = numpyro.deterministic(\n",
    "        \"k\", vmap_SE_kernel(X, X, kernel_var, kernel_length, kernel_noise)\n",
    "    )\n",
    "\n",
    "    with numpyro.plate(\"logw_plate\", M, dim=-1):\n",
    "        log_w = numpyro.sample(\n",
    "            \"w_un\", dist.MultivariateNormal(loc=-jnp.log(M), covariance_matrix=k)\n",
    "        )\n",
    "\n",
    "    ################################################\n",
    "    # Fuse with generalized multiplicative pooling #\n",
    "    ################################################\n",
    "    w = numpyro.deterministic(\"w\", jnp.exp(log_w))\n",
    "\n",
    "    tau_fused = numpyro.deterministic(\n",
    "        \"tau_fused\", jnp.einsum(\"nm,mn->n\", tau_preds, w)\n",
    "    )  # N,\n",
    "\n",
    "    assert tau_fused.shape == (N,)\n",
    "    mu_fused = numpyro.deterministic(\n",
    "        \"mean_fused\", jnp.einsum(\"nm,nm,mn->n\", tau_preds, mu_preds, w) / tau_fused\n",
    "    )  # N,\n",
    "    assert mu_fused.shape == (N,)\n",
    "    std_fused = numpyro.deterministic(\"std_fused\", 1 / jnp.sqrt(tau_fused))\n",
    "\n",
    "    numpyro.sample(\n",
    "        \"y_val\",\n",
    "        dist.Normal(loc=jnp.squeeze(mu_fused), scale=jnp.squeeze(std_fused)),\n",
    "        obs=y_val,\n",
    "    )\n",
    "\n",
    "    numpyro.deterministic(\n",
    "        \"lpd_point\",\n",
    "        jax.scipy.stats.norm.logpdf(\n",
    "        jnp.squeeze(y_val), loc=jnp.squeeze(mu_fused), scale=jnp.squeeze(std_fused))    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stacking(model=None):\n",
    "    mcmc = MCMC(\n",
    "    NUTS(model, \n",
    "         init_strategy=numpyro.infer.initialization.init_to_median,\n",
    "         ),\n",
    "    num_warmup=100,\n",
    "    num_samples=100,\n",
    "    num_chains=4,\n",
    ")\n",
    "\n",
    "    mcmc.run(random.PRNGKey(0), \n",
    "            X_val,   \n",
    "            mu_preds_val, \n",
    "            std_preds_val, \n",
    "            y_val=y_val, \n",
    "            )\n",
    "    mcmc.print_summary()\n",
    "    samples = mcmc.get_samples()\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def predict_stacking(model,samples,prior_mean = lambda x:  jnp.zeros(x.shape[0])):\n",
    "\n",
    "    res = vmapped_pred_with_mean(\n",
    "        random.PRNGKey(0),\n",
    "        X_val,\n",
    "        samples[\"w_un\"],\n",
    "        X_test, # TEST DATA\n",
    "        samples[\"kernel_var\"],\n",
    "        samples[\"kernel_length\"],\n",
    "        samples[\"kernel_noise\"],\n",
    "        SE_kernel,\n",
    "        prior_mean,\n",
    "    )\n",
    "\n",
    "\n",
    "    # w_un_samples = jnp.asarray(res[0] + np.random.randn(*res[0].shape) * res[1])\n",
    "    w_un_samples = jnp.asarray(res[0])\n",
    "    pred_samples = {\"w_un\": jnp.transpose(w_un_samples, (1, 0, 2))}\n",
    "\n",
    "    predictive = Predictive(model, pred_samples)\n",
    "    pred_samples = predictive(\n",
    "        random.PRNGKey(0),\n",
    "        X=X_test, # TEST DATA\n",
    "        mu_preds=mu_preds_test,  # TEST DATA\n",
    "        std_preds=std_preds_test, # TEST DATA\n",
    "        y_val = y_test, # TEST DATA\n",
    "    )\n",
    "\n",
    "    lpd_test = jax.nn.logsumexp(pred_samples[\"lpd_point\"],axis=0) - np.log(pred_samples[\"lpd_point\"].shape[0])\n",
    "\n",
    "    return pred_samples, lpd_test\n",
    "\n",
    "\n",
    "\n",
    "def get_ysamples(mus,stds,w):\n",
    "    def sample_mixture_gaussian(n_samples, means, std_devs, weights):    \n",
    "        n_components = len(means)\n",
    "        if not np.isclose(sum(weights), 1.0):\n",
    "            raise ValueError(\"Weights must sum to 1\") # Check that weights sum to 1\n",
    "        \n",
    "        if len(std_devs) != n_components:\n",
    "            raise ValueError(\"The number of means and standard deviations must be the same\")\n",
    "        \n",
    "        # Sample from a uniform distribution to decide which Gaussian to sample from\n",
    "        mixture_component = np.random.choice(n_components, size=n_samples, p=weights)\n",
    "        \n",
    "        # Allocate array for samples\n",
    "        samples = np.zeros(n_samples)\n",
    "        \n",
    "        # Sample from each Gaussian based on the mixture component\n",
    "        for i in range(n_components):\n",
    "            component_samples = np.random.normal(loc=means[i], scale=std_devs[i], size=np.sum(mixture_component == i))\n",
    "            samples[mixture_component == i] = component_samples\n",
    "        \n",
    "        return samples\n",
    "        \n",
    "    n_samples = 1000\n",
    "    ysamples = np.zeros((X_test.shape[0],n_samples))\n",
    "    for i in range(X_test.shape[0]):\n",
    "        means = mus[i,:]  # Means of the two Gaussians\n",
    "        std_devs = stds[i,:]  # Standard deviations of the two Gaussians\n",
    "        weights = w[i,:]  # Weights for the two Gaussians (must sum to 1)\n",
    "\n",
    "        ysamples[i,:] = sample_mixture_gaussian(n_samples, means, std_devs, weights)\n",
    "\n",
    "    return ysamples\n",
    "\n",
    "\n",
    "def plot_stacking(w, ysamples):\n",
    "    import matplotlib\n",
    "    # Define the colormap\n",
    "    cmap = matplotlib.colormaps['tab10']\n",
    "\n",
    "    # w = pred_samples[\"w\"].mean(0).T\n",
    "    plt.figure()\n",
    "    for i in range(w.shape[0]):    # plotting the weights of the experts\n",
    "        plt.plot(X_test, w[i,:], label = f\"weights GP{i}\",color=cmap(i))\n",
    "        # plt.legend()\n",
    "        plt.title(\"expert weights\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2, fontsize='small')    \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(X_test, ysamples.mean(1), label = \"stacking\",\n",
    "             color=\"magenta\",\n",
    "             linewidth=3)\n",
    "    plt.fill_between(X_test.squeeze(), \n",
    "                 np.percentile(ysamples,axis=1,q=97.5),\n",
    "                 np.percentile(ysamples,axis=1,q=2.5), \n",
    "                 color = \"magenta\",\n",
    "                 alpha = 0.4)  \n",
    "    for i in range(w.shape[0]):\n",
    "        plt.plot(X_test,mu_preds_test[:,i], label = f'GP{i}',color=cmap(i))\n",
    "        plt.fill_between(X_test.squeeze(), \n",
    "                         mu_preds_test[:,i] + 2*std_preds_test[:,i], # 95% CI\n",
    "                         mu_preds_test[:,i] - 2*std_preds_test[:,i],\n",
    "                         color=cmap(i), alpha = 0.15) \n",
    "    # plt.scatter(X_train, y_train, label = 'training obs')\n",
    "    plt.scatter(X_val, y_val, label = 'validation obs',color=cmap(i+1))\n",
    "    # plt.title(\"BHS\")\n",
    "    # plt.legend()     \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2, fontsize='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_phs = train_stacking(\n",
    "    model=mult_stack_gp_w_model,\n",
    "    # model = mult_stack_gp_w_model_normalized_w,\n",
    "    )\n",
    "preds_phs, lpd_phs_test = predict_stacking(\n",
    "                                           model = mult_stack_gp_w_model,\n",
    "                                          #  model = mult_stack_gp_w_model_normalized_w,\n",
    "                                           samples=samples_phs,\n",
    "                                           prior_mean=lambda x: -jnp.log(mu_preds_test.shape[1]) * jnp.ones(x.shape[0]),\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-lpd_phs_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_phs[\"w\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_experts):\n",
    "    plt.plot(preds_phs[\"w\"].mean(0)[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhs(X, mu_preds, std_preds, y_val=None):\n",
    "    N, M = mu_preds.shape\n",
    "\n",
    "    assert mu_preds.shape == std_preds.shape\n",
    "\n",
    "    ######################\n",
    "    # GP for log weights #\n",
    "    ######################\n",
    "    with numpyro.plate(\"M\", M):\n",
    "        kernel_var = numpyro.sample(\"kernel_var\", dist.HalfNormal(1.0))\n",
    "        kernel_length = numpyro.sample(\n",
    "            \"kernel_length\", dist.InverseGamma(5.0, 5.0))\n",
    "        kernel_noise = numpyro.sample(\"kernel_noise\", dist.HalfNormal(1.0))\n",
    "\n",
    "    k = numpyro.deterministic(\n",
    "        \"k\", vmap_SE_kernel(X, X, kernel_var, kernel_length, kernel_noise)\n",
    "    )\n",
    "\n",
    "    with numpyro.plate(\"logw_plate\", M, dim=-1):\n",
    "        w_un = numpyro.sample(\n",
    "            \"w_un\", dist.MultivariateNormal(\n",
    "                loc=-jnp.log(M), covariance_matrix=k)\n",
    "        )\n",
    "\n",
    "    log_w = jax.nn.log_softmax(w_un.T, axis=1)\n",
    "\n",
    "    #################\n",
    "    # Fuse with BHS #\n",
    "    #################\n",
    "    y_val_rep = jnp.tile(jnp.reshape(y_val, (-1, 1)), M)\n",
    "    lpd_point = jax.scipy.stats.norm.logpdf(\n",
    "        y_val_rep, loc=mu_preds, scale=std_preds)\n",
    "    logp = jax.nn.logsumexp(lpd_point + log_w, axis=1)\n",
    "    numpyro.deterministic(\"lpd_point\", logp)\n",
    "    numpyro.deterministic(\"w\", jnp.exp(log_w))\n",
    "    numpyro.factor(\"logp\", jnp.sum(logp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_bhs = train_stacking(model=bhs)\n",
    "preds_bhs, lpd_bhs_test = predict_stacking(bhs,samples=samples_bhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-lpd_bhs_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_bhs[\"w\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_experts):\n",
    "    plt.plot(preds_bhs[\"w\"].mean(0)[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, MAP (Autodelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "print(isinstance(chain(clip(10.0), adam(0.005)), pyro.optim.PyroOptim))\n",
    "print(isinstance(numpyro.optim.Adam(0.1), pyro.optim.PyroOptim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi= SVI(mult_stack_gp_w_model\n",
    "        ,\n",
    "        AutoDelta(mult_stack_gp_w_model, \n",
    "                  init_loc_fn = numpyro.infer.initialization.init_to_median\n",
    "                  # init_loc_fn = numpyro.infer.initialization.init_to_uniform),\n",
    "                  ),\n",
    "        # optim = numpyro.optim.Minimize(),  # Using lbfgs instead of adam (overfitting is huge with this optimizer...)\n",
    "        # optim=chain(clip(10.0), adam(0.005)),\n",
    "        # optim = adam(0.1),\n",
    "        # optim=numpyro.optim.Adagrad(0.1),\n",
    "        optim = X_test,\n",
    "        loss=Trace_ELBO(),\n",
    "    )\n",
    "\n",
    "\n",
    "res_phs = svi.run(\n",
    "    random.PRNGKey(0),\n",
    "    # 20,\n",
    "    5000,  # for adam\n",
    "    X_val,   \n",
    "    mu_preds_val, \n",
    "    std_preds_val, \n",
    "    y_val=y_val,\n",
    ")\n",
    "plt.plot(res_phs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = res_phs.params\n",
    "guide = AutoDelta(mult_stack_gp_w_model)\n",
    "# use guide to make predictive\n",
    "svi_predictive = Predictive(model=mult_stack_gp_w_model, \n",
    "                                       guide=guide, params=params, num_samples=1) # Since we are using the MAP, setting num_samples greater than 1 is pointless.\n",
    "'''\n",
    "below are predictions using the MAP estimates of all the parameters (it's only useful when predicting \n",
    "at the same x-values that were used for training the model; otherwise, it'll throw an error)\n",
    "'''\n",
    "point_estimates_at_val = svi_predictive(random.PRNGKey(0), \n",
    "                                                        X_val,   \n",
    "                                                        mu_preds_val, \n",
    "                                                        std_preds_val, \n",
    "                                                        y_val=y_val,\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_estimates_at_val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-point_estimates_at_val[\"lpd_point\"].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we can compare the above lpd value with the one from MCMC (both on validation data)\n",
    "'''\n",
    "-np.mean(\n",
    "    jax.nn.logsumexp(samples_phs[\"lpd_point\"],axis=0) - np.log(samples_phs[\"lpd_point\"].shape[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create a sample dictionary with the correct names\n",
    "# we need to include a dim of legnth 1 at the beginning for being able to use the predict_stacking\n",
    "samples_phs_svi = {}\n",
    "for key, value in params.items():\n",
    "    # print(key)\n",
    "    new_key = key.replace(\"_auto_loc\",\"\")\n",
    "    samples_phs_svi[f\"{new_key}\"] = value\n",
    "    samples_phs_svi[f\"{new_key}\"] = samples_phs_svi[f\"{new_key}\"][np.newaxis]\n",
    "    # print(new_key)\n",
    "    # print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_phs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_phs_svi.keys() # these are the only params we care about for when using predict_stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_phs_svi[\"kernel_length\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forma alternativa para obtener muestras (aunque todas seran iguales porque estamos usando el MAP)\n",
    "guide = AutoDelta(mult_stack_gp_w_model)\n",
    "predictive = Predictive(guide, params=params, num_samples=2)\n",
    "posterior_samples = predictive(random.PRNGKey(1),X_val,mu_preds_val,std_preds_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples[\"kernel_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_phs_svi, lpd_phs_test_svi = predict_stacking(\n",
    "                                           model = mult_stack_gp_w_model,\n",
    "                                           samples=samples_phs_svi,\n",
    "                                           prior_mean=lambda x: -jnp.log(mu_preds_test.shape[1]) * jnp.ones(x.shape[0]),\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NLPD MCMC\", -lpd_phs_test.mean(0))\n",
    "print(\"NLPD map\", -lpd_phs_test_svi.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_experts):\n",
    "    plt.plot(preds_phs_svi[\"w\"].mean(0)[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_experts):\n",
    "    plt.plot(point_estimates_at_val[\"w\"].mean(0)[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_phs_svi[\"w\"][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi= SVI(bhs\n",
    "        ,\n",
    "        AutoDelta(bhs, \n",
    "                  init_loc_fn = numpyro.infer.initialization.init_to_median\n",
    "                  # init_loc_fn = numpyro.infer.initialization.init_to_uniform),\n",
    "                  ),\n",
    "        # optim = numpyro.optim.Minimize(),  # Using lbfgs instead of adam (overfitting is huge with this optimizer...)\n",
    "        optim=chain(clip(10.0), adam(0.005)),\n",
    "        # optim=numpyro.optim.\n",
    "        # optim=adam(0.001),\n",
    "        loss=Trace_ELBO(),\n",
    "    )\n",
    "\n",
    "\n",
    "res_bhs = svi.run(\n",
    "    random.PRNGKey(0),\n",
    "    # 20,\n",
    "    3000,  # for adam\n",
    "    X_val,   \n",
    "    mu_preds_val, \n",
    "    std_preds_val, \n",
    "    y_val=y_val,\n",
    ")\n",
    "plt.plot(res_bhs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = res_bhs.params\n",
    "guide = AutoDelta(bhs)\n",
    "# use guide to make predictive\n",
    "svi_predictive = Predictive(model=bhs,guide=guide, params=params, num_samples=1) # Since we are using the MAP, setting num_samples greater than 1 is pointless.\n",
    "'''\n",
    "below are predictions using the MAP estimates of all the parameters (it's only useful when predicting \n",
    "at the same x-values that were used for training the model; otherwise, it'll throw an error)\n",
    "'''\n",
    "point_estimates_at_val = svi_predictive(random.PRNGKey(0), \n",
    "                                                        X_val,   \n",
    "                                                        mu_preds_val, \n",
    "                                                        std_preds_val, \n",
    "                                                        y_val=y_val,\n",
    "                                                        )\n",
    "print(-point_estimates_at_val[\"lpd_point\"].mean(1))\n",
    "'''\n",
    "we can compare the above lpd value with the one from MCMC (both on validation data)\n",
    "'''\n",
    "print(\n",
    "    -np.mean(\n",
    "    jax.nn.logsumexp(samples_bhs[\"lpd_point\"],axis=0) - np.log(samples_bhs[\"lpd_point\"].shape[0])\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create a sample dictionary with the correct names\n",
    "# we need to include a dim of legnth 1 at the beginning for being able to use the predict_stacking\n",
    "samples_bhs_svi = {}\n",
    "for key, value in params.items():\n",
    "    # print(key)\n",
    "    new_key = key.replace(\"_auto_loc\",\"\")\n",
    "    samples_bhs_svi[f\"{new_key}\"] = value\n",
    "    samples_bhs_svi[f\"{new_key}\"] = samples_bhs_svi[f\"{new_key}\"][np.newaxis]\n",
    "    # print(new_key)\n",
    "    # print(value)\n",
    "preds_bhs_svi, lpd_bhs_test_svi = predict_stacking(\n",
    "                                           model = bhs,\n",
    "                                           samples=samples_bhs_svi,\n",
    "                                        #    prior_mean=lambda x: -jnp.log(mu_preds_test.shape[1]) * jnp.ones(x.shape[0]),\n",
    "                                           )\n",
    "print(\"NLPD MCMC\", -lpd_bhs_test.mean(0))\n",
    "print(\"NLPD map\", -lpd_bhs_test_svi.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_bhs_svi[\"w\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_experts):\n",
    "    plt.plot(preds_bhs_svi[\"w\"].mean(0).T[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second, Gaussian approx (AutoNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer.autoguide import AutoNormal\n",
    "\n",
    "svi= SVI(mult_stack_gp_w_model\n",
    "        ,\n",
    "        AutoNormal(mult_stack_gp_w_model, \n",
    "                  init_loc_fn = numpyro.infer.initialization.init_to_median\n",
    "                  # init_loc_fn = numpyro.infer.initialization.init_to_uniform),\n",
    "                  ),\n",
    "        # optim = numpyro.optim.Minimize(),  # Using lbfgs instead of adam (overfitting is huge with this optimizer...)\n",
    "        optim=chain(clip(10.0), adam(0.005)),\n",
    "        # optim=numpyro.optim.\n",
    "        # optim=adam(0.001),\n",
    "        loss=Trace_ELBO(),\n",
    "    )\n",
    "\n",
    "\n",
    "res_phs_normal = svi.run(\n",
    "    random.PRNGKey(0),\n",
    "    # 20,\n",
    "    3000,  # for adam\n",
    "    X_val,   \n",
    "    mu_preds_val, \n",
    "    std_preds_val, \n",
    "    y_val=y_val,\n",
    ")\n",
    "plt.plot(res_phs_normal[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = res_phs_normal.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to sample from this Gaussian approx to the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = AutoNormal(mult_stack_gp_w_model)\n",
    "predictive = Predictive(guide, params=params, num_samples=400)\n",
    "posterior_samples = predictive(random.PRNGKey(1),X_val,mu_preds_val,std_preds_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples[\"kernel_var\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we make predictions using these samples\n",
    "\n",
    "preds_phs_normal, lpd_phs_test_normal = predict_stacking(\n",
    "                                           model = mult_stack_gp_w_model,\n",
    "                                           samples=posterior_samples,\n",
    "                                           prior_mean=lambda x: -jnp.log(mu_preds_test.shape[1]) * jnp.ones(x.shape[0]),\n",
    "                                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NLPD MCMC\", -lpd_phs_test.mean(0))\n",
    "print(\"NLPD map\", -lpd_phs_test_svi.mean(0))\n",
    "print(\"NLPD GaussApprox\", -lpd_phs_test_normal.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_phs_normal[\"w\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_experts):\n",
    "    plt.plot(preds_phs_normal[\"w\"].mean(0)[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer.autoguide import AutoNormal\n",
    "\n",
    "svi= SVI(bhs\n",
    "        ,\n",
    "        AutoNormal(bhs, \n",
    "                  init_loc_fn = numpyro.infer.initialization.init_to_median\n",
    "                  # init_loc_fn = numpyro.infer.initialization.init_to_uniform),\n",
    "                  ),\n",
    "        # optim = numpyro.optim.Minimize(),  # Using lbfgs instead of adam (overfitting is huge with this optimizer...)\n",
    "        optim=chain(clip(10.0), adam(0.005)),\n",
    "        # optim=numpyro.optim.\n",
    "        # optim=adam(0.001),\n",
    "        loss=Trace_ELBO(),\n",
    "    )\n",
    "\n",
    "\n",
    "res_bhs_normal = svi.run(\n",
    "    random.PRNGKey(0),\n",
    "    # 20,\n",
    "    3000,  # for adam\n",
    "    X_val,   \n",
    "    mu_preds_val, \n",
    "    std_preds_val, \n",
    "    y_val=y_val,\n",
    ")\n",
    "plt.plot(res_bhs_normal[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = AutoNormal(bhs)\n",
    "predictive = Predictive(guide, params=params, num_samples=400)\n",
    "posterior_samples = predictive(random.PRNGKey(1),X_val,mu_preds_val,std_preds_val,y_val)\n",
    "\n",
    "# And we make predictions using these samples\n",
    "\n",
    "preds_bhs_normal, lpd_bhs_test_normal = predict_stacking(\n",
    "                                           model = bhs,\n",
    "                                           samples=posterior_samples,\n",
    "                                        #    prior_mean=lambda x: -jnp.log(mu_preds_test.shape[1]) * jnp.ones(x.shape[0]),\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NLPD MCMC\", -lpd_bhs_test.mean(0))\n",
    "print(\"NLPD map\", -lpd_bhs_test_svi.mean(0))\n",
    "print(\"NLPD GaussApprox\", -lpd_bhs_test_normal.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_experts):\n",
    "    plt.plot(preds_bhs_normal[\"w\"].mean(0).T[i,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
