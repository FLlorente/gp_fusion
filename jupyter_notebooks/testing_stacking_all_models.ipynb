{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/fllorente/Dropbox/con_Petar/PYTHON/gp_fusion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data_handling import load_and_normalize_data, split_dataset, create_validation_set\n",
    "from modules.prediction_storage import store_predictions\n",
    "from modules.fusion_methods import compute_neg_log_like, product_fusion, train_and_predict_fusion_method\n",
    "from modules.model_training import train_and_predict_single_gp, train_expert, store_predictions_for_experts\n",
    "from modules.phs import phs\n",
    "from modules.bhs import bhs\n",
    "from modules.model_training import train_joint_experts_shared_kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size:  567\n",
      "test size:  63\n"
     ]
    }
   ],
   "source": [
    "# ------------ Load and normalize data --------- #\n",
    "# dataset_name = \"concrete\"\n",
    "dataset_name = \"pendulum\"\n",
    "split=3\n",
    "X_train, y_train, X_test, y_test = load_and_normalize_data(dataset_name, \n",
    "                                                           split,\n",
    "                                                           normalize_x_method=\"max-min\",\n",
    "                                                           normalize_y=True)\n",
    "\n",
    "print(\"training size: \", len(y_train))\n",
    "print(\"test size: \", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experts are trained on all training data and validation data is taken from training splits\n"
     ]
    }
   ],
   "source": [
    "n_experts = 3\n",
    "val_incest = True\n",
    "\n",
    "if val_incest:\n",
    "    print(\"Experts are trained on all training data and validation data is taken from training splits\")\n",
    "    # we create the splits using all training data\n",
    "    splits = split_dataset(X_train, y_train, n_splits=n_experts, with_replacement=False)\n",
    "    # we create the validation from the same training data (= data incest)\n",
    "    n_points_per_split = 5\n",
    "    X_val, y_val = create_validation_set(splits, n_points_per_split)\n",
    "else:\n",
    "    print(\"Training data is split into two: one for training the experts and the other for the weights\")\n",
    "    validation_proportion = 0.1\n",
    "    num_val_samples = int(validation_proportion*len(X_train))\n",
    "\n",
    "    indices = np.arange(len(X_train))\n",
    "    np.random.seed(11)\n",
    "    np.random.shuffle(indices)  # Shuffle the data indices\n",
    "    val_indices = indices[:num_val_samples]\n",
    "    train_indices = indices[num_val_samples:]\n",
    "\n",
    "    # training data is split into two so that we don't use twice the data for two learning/training stages (experts' training and the weight training)\n",
    "    X_train_train = X_train[train_indices].copy()\n",
    "    y_train_train = y_train[train_indices].copy()\n",
    "\n",
    "    X_val = X_train[val_indices].copy()\n",
    "    y_val = y_train[val_indices].copy()\n",
    "\n",
    "    splits = split_dataset(X_train_train, y_train_train, n_splits=n_experts, with_replacement=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n",
      "189\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "for split in splits:\n",
    "    print(split[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experts are trained independently\n",
      "[0.59293142 0.52098088 0.76588723] , average NLPD:  0.6265998444240394\n"
     ]
    }
   ],
   "source": [
    "kappa = 2\n",
    "lambdaa = 2\n",
    "training_iter = 200\n",
    "lr = 0.01\n",
    "\n",
    "joint_training = False\n",
    "\n",
    "if joint_training:\n",
    "    print(\"Experts are trained jointly\")\n",
    "    # ====== for joint training ========== #\n",
    "    models, likelihood = train_joint_experts_shared_kernel(splits, kappa, lambdaa,\n",
    "                                                           lr=lr,\n",
    "                                                           training_iter=training_iter)\n",
    "    experts = [(model,likelihood) for model in models]\n",
    "    # ====== for independent training ==== #\n",
    "else:\n",
    "    print(\"Experts are trained independently\")\n",
    "    experts = []\n",
    "    for X_split, y_split in splits:\n",
    "        model, likelihood = train_expert(X_split, y_split, kappa, lambdaa,\n",
    "                                         lr=lr,\n",
    "                                         training_iter=training_iter)\n",
    "        experts.append((model, likelihood))\n",
    "\n",
    "\n",
    "# Store predictions for experts on the test set\n",
    "mu_preds_test, std_preds_test, std_preds_prior_test = store_predictions_for_experts(experts, X_test)\n",
    "\n",
    "# Compute negative log likelihood for experts\n",
    "nlpd_experts = compute_neg_log_like(mu_preds_test, std_preds_test, y_test)\n",
    "print(nlpd_experts, \", average NLPD: \",nlpd_experts.mean())\n",
    "\n",
    "# Store predictions for experts on the validation set\n",
    "mu_preds_val, std_preds_val, _ = store_predictions_for_experts(experts, X_val) # we don't need the prior predictive variances here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full GP:  -0.9710924335998848\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# seed = 100\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "lr = 0.1\n",
    "training_iter = 100\n",
    "kappa = 5\n",
    "lambdaa = 5\n",
    "\n",
    "# ---------  Single GP using all training data ----- #\n",
    "test_preds, _ = train_and_predict_single_gp(X_train, y_train, X_test, X_val, kappa, lambdaa,\n",
    "                                            lr=lr,\n",
    "                                            training_iter=training_iter,\n",
    "                                            )\n",
    "nlpd_single_gp = compute_neg_log_like(test_preds.mean.numpy().reshape(-1, 1), \n",
    "                                      np.sqrt(test_preds.variance.numpy().reshape(-1, 1)), y_test)\n",
    "print(\"full GP: \", nlpd_single_gp.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the different stacking algorithms:\n",
    "PHS with MCMC or VI (AutoDelta or AutoNormal)\n",
    "\n",
    "BHS with MCMC or VI (AutoDelta or AutoNormal)\n",
    "\n",
    "RFF-based PHS with MCMC or VI (coming soon...)\n",
    "\n",
    "RFF-based BHS with MCMC or VI (coming soon...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCMC with NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, lpd = train_and_predict_fusion_method(\n",
    "                                phs,\n",
    "                                X_val,\n",
    "                                mu_preds_val,\n",
    "                                std_preds_val,\n",
    "                                y_val,\n",
    "                                X_test,\n",
    "                                mu_preds_test,\n",
    "                                std_preds_test,\n",
    "                                y_test,\n",
    "                                method=\"mcmc\",\n",
    "                                parallel_mcmc=True,\n",
    "                                show_progress=True,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[\"w\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpd_phs_mcmc = -lpd.mean()\n",
    "for i in range(n_experts):\n",
    "    plt.plot(preds[\"w\"].mean(0)[i,:])\n",
    "print(\"PHS with MCMC: \", nlpd_phs_mcmc)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP with SVI AutoDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, lpd = train_and_predict_fusion_method(\n",
    "                                phs,\n",
    "                                X_val,\n",
    "                                mu_preds_val,\n",
    "                                std_preds_val,\n",
    "                                y_val,\n",
    "                                X_test,\n",
    "                                mu_preds_test,\n",
    "                                std_preds_test,\n",
    "                                y_test,\n",
    "                                method=\"svi\",\n",
    "                                guide_svi=\"map\",\n",
    "                                show_progress=True,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[\"w\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpd_phs_map = -lpd.mean()\n",
    "for i in range(n_experts):\n",
    "    plt.plot(preds[\"w\"].mean(0)[i,:])\n",
    "print(\"PHS with MAP: \", nlpd_phs_map)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate Gaussian posterior with SVI AutoNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, lpd = train_and_predict_fusion_method(\n",
    "                                phs,\n",
    "                                X_val,\n",
    "                                mu_preds_val,\n",
    "                                std_preds_val,\n",
    "                                y_val,\n",
    "                                X_test,\n",
    "                                mu_preds_test,\n",
    "                                std_preds_test,\n",
    "                                y_test,\n",
    "                                method=\"svi\",\n",
    "                                guide_svi=\"normal\",\n",
    "                                show_progress=True,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[\"w\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpd_phs_normal = -lpd.mean()\n",
    "for i in range(n_experts):\n",
    "    plt.plot(preds[\"w\"].mean(0)[i,:])\n",
    "print(\"PHS with Normal: \", nlpd_phs_normal)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCMC with NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, lpd = train_and_predict_fusion_method(\n",
    "                                bhs,\n",
    "                                X_val,\n",
    "                                mu_preds_val,\n",
    "                                std_preds_val,\n",
    "                                y_val,\n",
    "                                X_test,\n",
    "                                mu_preds_test,\n",
    "                                std_preds_test,\n",
    "                                y_test,\n",
    "                                method=\"mcmc\",\n",
    "                                parallel_mcmc=True,\n",
    "                                show_progress=True,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[\"w\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpd_bhs_mcmc = -lpd.mean()\n",
    "for i in range(n_experts):\n",
    "    plt.plot(preds[\"w\"].mean(0).T[i,:])\n",
    "print(\"BHS with MCMC: \", nlpd_bhs_mcmc)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP with SVI AutoDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, lpd = train_and_predict_fusion_method(\n",
    "                                bhs,\n",
    "                                X_val,\n",
    "                                mu_preds_val,\n",
    "                                std_preds_val,\n",
    "                                y_val,\n",
    "                                X_test,\n",
    "                                mu_preds_test,\n",
    "                                std_preds_test,\n",
    "                                y_test,\n",
    "                                method=\"svi\",\n",
    "                                guide_svi=\"map\",\n",
    "                                show_progress=True,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[\"w\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpd_bhs_map = -lpd.mean()\n",
    "for i in range(n_experts):\n",
    "    plt.plot(preds[\"w\"].mean(0).T[i,:])\n",
    "print(\"BHS with MAP: \", nlpd_bhs_map)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate Gaussian posterior with SVI AutoNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, lpd = train_and_predict_fusion_method(\n",
    "                                bhs,\n",
    "                                X_val,\n",
    "                                mu_preds_val,\n",
    "                                std_preds_val,\n",
    "                                y_val,\n",
    "                                X_test,\n",
    "                                mu_preds_test,\n",
    "                                std_preds_test,\n",
    "                                y_test,\n",
    "                                method=\"svi\",\n",
    "                                guide_svi=\"normal\",\n",
    "                                show_progress=True,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[\"w\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpd_bhs_normal = -lpd.mean()\n",
    "for i in range(n_experts):\n",
    "    plt.plot(preds[\"w\"].mean(0).T[i,:])\n",
    "print(\"BHS with Normal: \", nlpd_bhs_normal)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
