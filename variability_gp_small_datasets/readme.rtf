{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red140\green211\blue254;\red24\green24\blue24;\red193\green193\blue193;
\red202\green202\blue202;\red89\green138\blue67;\red194\green126\blue101;\red167\green197\blue152;}
{\*\expandedcolortbl;;\cssrgb\c61176\c86275\c99608;\cssrgb\c12157\c12157\c12157;\cssrgb\c80000\c80000\c80000;
\cssrgb\c83137\c83137\c83137;\cssrgb\c41569\c60000\c33333;\cssrgb\c80784\c56863\c47059;\cssrgb\c70980\c80784\c65882;}
\margl1440\margr1440\vieww16360\viewh11220\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 In analyzing the variability of a single GP performance (in terms of NLPD) I compared the following training configurations:\
\
\pard\pardeftab720\partightenfactor0

\f1\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
configs\cf4  \cf5 =\cf4  [  \cb1 \
\cb3     \cf6 # Low Noise, Short Lengthscale Regime\cf4 \cb1 \
\cb3     \{\cf7 "kappa"\cf4 : \cf8 50\cf4 , \cf7 "lambdaa"\cf4 : \cf8 10\cf4 , \cf7 "lr"\cf4 : \cf8 0.1\cf4 , \cf7 "training_iter"\cf4 : \cf8 100\cf4 \},\cb1 \
\cb3     \{\cf7 "kappa"\cf4 : \cf8 50\cf4 , \cf7 "lambdaa"\cf4 : \cf8 10\cf4 , \cf7 "lr"\cf4 : \cf8 0.01\cf4 , \cf7 "training_iter"\cf4 : \cf8 500\cf4 \},\cb1 \
\cb3     \{\cf7 "kappa"\cf4 : \cf8 50\cf4 , \cf7 "lambdaa"\cf4 : \cf8 8\cf4 , \cf7 "lr"\cf4 : \cf8 0.1\cf4 , \cf7 "training_iter"\cf4 : \cf8 100\cf4 \},\cb1 \
\cb3     \{\cf7 "kappa"\cf4 : \cf8 50\cf4 , \cf7 "lambdaa"\cf4 : \cf8 8\cf4 , \cf7 "lr"\cf4 : \cf8 0.01\cf4 , \cf7 "training_iter"\cf4 : \cf8 500\cf4 \},\cb1 \
\cb3     \cb1 \
\cb3     \cf6 # High Noise, Long Lengthscale Regime\cf4 \cb1 \
\cb3     \{\cf7 "kappa"\cf4 : \cf8 2\cf4 , \cf7 "lambdaa"\cf4 : \cf8 1\cf4 , \cf7 "lr"\cf4 : \cf8 0.1\cf4 , \cf7 "training_iter"\cf4 : \cf8 100\cf4 \},\cb1 \
\cb3     \{\cf7 "kappa"\cf4 : \cf8 2\cf4 , \cf7 "lambdaa"\cf4 : \cf8 1\cf4 , \cf7 "lr"\cf4 : \cf8 0.01\cf4 , \cf7 "training_iter"\cf4 : \cf8 500\cf4 \},\cb1 \
\cb3     \{\cf7 "kappa"\cf4 : \cf8 5\cf4 , \cf7 "lambdaa"\cf4 : \cf8 2\cf4 , \cf7 "lr"\cf4 : \cf8 0.1\cf4 , \cf7 "training_iter"\cf4 : \cf8 100\cf4 \},\cb1 \
\cb3     \{\cf7 "kappa"\cf4 : \cf8 5\cf4 , \cf7 "lambdaa"\cf4 : \cf8 2\cf4 , \cf7 "lr"\cf4 : \cf8 0.01\cf4 , \cf7 "training_iter"\cf4 : \cf8 500\cf4 \},\cb1 \
\cb3     \cb1 \
\cb3     \cf6 # Balanced Regime\cf4 \cb1 \
\cb3     \{\cf7 "kappa"\cf4 : \cf8 10\cf4 , \cf7 "lambdaa"\cf4 : \cf8 5\cf4 , \cf7 "lr"\cf4 : \cf8 0.1\cf4 , \cf7 "training_iter"\cf4 : \cf8 100\cf4 \},\cb1 \
\cb3     \{\cf7 "kappa"\cf4 : \cf8 10\cf4 , \cf7 "lambdaa"\cf4 : \cf8 5\cf4 , \cf7 "lr"\cf4 : \cf8 0.01\cf4 , \cf7 "training_iter"\cf4 : \cf8 500\cf4 \},\cb1 \
\cb3     \{\cf7 "kappa"\cf4 : \cf8 20\cf4 , \cf7 "lambdaa"\cf4 : \cf8 5\cf4 , \cf7 "lr"\cf4 : \cf8 0.1\cf4 , \cf7 "training_iter"\cf4 : \cf8 100\cf4 \},\cb1 \
\cb3     \{\cf7 "kappa"\cf4 : \cf8 20\cf4 , \cf7 "lambdaa"\cf4 : \cf8 5\cf4 , \cf7 "lr"\cf4 : \cf8 0.01\cf4 , \cf7 "training_iter"\cf4 : \cf8 500\cf4 \},\cb1 \
\cb3 ]\cb1 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \kerning1\expnd0\expndtw0 Hence, in the plots, the enumerated Config correspond to the cases above: Config 1 in the first one, Config 2 is the second one, and so on.\
I was using max-min normalization for X and z-score normalization for Y.}